{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0jbEcRSrlps"
   },
   "source": [
    "# Convolutional Neural Network\n",
    "**get datasets from udemy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yOPvB-rOrlpx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "uAuoBMsdsipJ",
    "outputId": "a6e948f8-9c65-49b5-b925-4bff7fe59fad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09XEspMfsnwV"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataset\n",
    "- **target_size** and batch_size of training and testing should be same\n",
    "\n",
    "- **target_size**(images from cat folder, images from dog folder)\n",
    "\n",
    "- **batch size** defines the number of samples that will be propagated through the network.\n",
    "\n",
    "- For instance, let's say you have 1050 training samples and you want to set up a batch_size equal to 100. The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network. Next, it takes the second 100 samples (from 101st to 200th) and trains the network again. We can keep doing this procedure until we have propagated all samples through of the network\n",
    "\n",
    "- 32 is the default value of batch_size\n",
    "\n",
    "- **class_mode = 'binary**  \n",
    "-- when we have only 2 classes to predict (let say cat and dog)\n",
    "\n",
    "- **class_mode = 'classification'**  \n",
    "-- when we have more than 2 classes to predict (let say cat,dog and elephant)\n",
    "\n",
    "- **1./255 divide all pixels with 255 (converts all pixels values between 0 and 1)**\n",
    "\n",
    "- **shear_range , zoom_range and horizontal_flip** used for image augmentation (used to avid overfitting)\n",
    "\n",
    "- Image Augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset\n",
    "\n",
    "- Dont apply image augmentation on test set as testing should be on original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_data_gen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "training_data = training_data_gen.flow_from_directory(\n",
    "    '../../Datasets/cnn/cnn_training_data', target_size=(64, 64), batch_size=32, class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_data = training_data_gen.flow_from_directory(\n",
    "    '../../Datasets/cnn/cnn_test_data', target_size=(64, 64), batch_size=32, class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPsYP4KdvWjj"
   },
   "source": [
    "## Building the CNN\n",
    "- **reshape** used for feature scaling of images\n",
    "\n",
    "- feature scaling is compulsory for training neural networks\n",
    "\n",
    "- **kernel_size** used to define matrix size for feature detection filters (32 is recommended) \n",
    "- **filters** used to define filters in the features of images \n",
    "- **activation** used to define activation function \n",
    "- **input_shape[same size of training and test target size, 3 for color images/1 for black and white images]** used to define shape of input images \n",
    "- **pool_size** used to define max pooling matrix size in feature map \n",
    "- **strides** used to define next max pooling matrix within feature map \n",
    "- **input_shape should only be defined in first convolution layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5y6F381WvbE1"
   },
   "source": [
    "### Initializing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dxEZrQ05vYgc"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDR8Yr1evjxO"
   },
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6hOKBTOHvkUf"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOQJRDfUvzdu"
   },
   "source": [
    "### Pooling\n",
    "- pooling is mainly used to prevent overfitting by removing extra features from the image\n",
    "\n",
    "- pooling reduces the image size\n",
    "\n",
    "- pooling is used to get invariants(tilts, twists etc) of the image instead of ideal scenario. but pooling prserved all main featured of the image which is called max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gGbaZPH-vz2n"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKlKR3tgv2ZS"
   },
   "source": [
    "### Adding the output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_K4-BsGMv2x6"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hA7mINAowBY-"
   },
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "34gkB-cTwDNb"
   },
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_cZtKr1wYx9"
   },
   "source": [
    "### Training the CNN on the training data and evaluating it on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KjqnDcCTwbXW",
    "outputId": "89353941-b2af-484f-8861-7074f9e88947"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 42s 167ms/step - loss: 0.6775 - accuracy: 0.5770 - val_loss: 0.7067 - val_accuracy: 0.5500\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 41s 163ms/step - loss: 0.6147 - accuracy: 0.6634 - val_loss: 0.5783 - val_accuracy: 0.7020\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 40s 162ms/step - loss: 0.5638 - accuracy: 0.7054 - val_loss: 0.5483 - val_accuracy: 0.7305\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 41s 162ms/step - loss: 0.5358 - accuracy: 0.7266 - val_loss: 0.6160 - val_accuracy: 0.6505\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 0.5160 - accuracy: 0.7436 - val_loss: 0.5915 - val_accuracy: 0.6920\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 41s 162ms/step - loss: 0.4906 - accuracy: 0.7604 - val_loss: 0.5159 - val_accuracy: 0.7375\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 41s 162ms/step - loss: 0.4817 - accuracy: 0.7634 - val_loss: 0.4954 - val_accuracy: 0.7730\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 41s 163ms/step - loss: 0.4662 - accuracy: 0.7760 - val_loss: 0.4840 - val_accuracy: 0.7715\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 41s 163ms/step - loss: 0.4509 - accuracy: 0.7883 - val_loss: 0.4840 - val_accuracy: 0.7665\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 50s 199ms/step - loss: 0.4429 - accuracy: 0.7925 - val_loss: 0.4889 - val_accuracy: 0.7690\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 0.4367 - accuracy: 0.7894 - val_loss: 0.4765 - val_accuracy: 0.7780\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 0.4197 - accuracy: 0.8067 - val_loss: 0.4755 - val_accuracy: 0.7780\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 45s 180ms/step - loss: 0.4003 - accuracy: 0.8179 - val_loss: 0.4633 - val_accuracy: 0.7880\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 0.3954 - accuracy: 0.8202 - val_loss: 0.4876 - val_accuracy: 0.7665\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.3894 - accuracy: 0.8231 - val_loss: 0.4777 - val_accuracy: 0.7760\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.3761 - accuracy: 0.8303 - val_loss: 0.4588 - val_accuracy: 0.7990\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 41s 162ms/step - loss: 0.3631 - accuracy: 0.8420 - val_loss: 0.4515 - val_accuracy: 0.7990\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 42s 166ms/step - loss: 0.3537 - accuracy: 0.8419 - val_loss: 0.4558 - val_accuracy: 0.7915\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.3484 - accuracy: 0.8445 - val_loss: 0.4517 - val_accuracy: 0.7980\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.3414 - accuracy: 0.8501 - val_loss: 0.4746 - val_accuracy: 0.7965\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.3241 - accuracy: 0.8576 - val_loss: 0.4540 - val_accuracy: 0.8030\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.3215 - accuracy: 0.8616 - val_loss: 0.4939 - val_accuracy: 0.7820\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.3161 - accuracy: 0.8593 - val_loss: 0.5382 - val_accuracy: 0.7880\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 42s 166ms/step - loss: 0.2963 - accuracy: 0.8736 - val_loss: 0.4651 - val_accuracy: 0.8080\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.2916 - accuracy: 0.8792 - val_loss: 0.4811 - val_accuracy: 0.7895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa5d39d1a90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=training_data, validation_data=test_data, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xvu-DgGsy4o9"
   },
   "source": [
    "## Prediction on user data\n",
    "- **expand_dims** used to handle batches target_size of prediction should be same as training and testing target_size used to define size of input image (previous definition is wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "GmtTjnrexzEA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('../../Datasets/cnn/cnn_user_test_image_dog.jpg', target_size=(64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "prediction = cnn.predict(test_image)\n",
    "training_data.class_indices  # used to get indices for prediction classed i.e 0=cat, 1=dog used by model\n",
    "if prediction[0][0] == 1:\n",
    "    print('Dog')\n",
    "else:\n",
    "    print('Cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
